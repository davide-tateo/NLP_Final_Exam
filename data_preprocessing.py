# -*- coding: utf-8 -*-
"""Data Preprocessing and LDA Topic Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rS2HDq8e4uCmKkm5N5hp13rUqJ67t4Uj
"""

# Importing all the necessary packages
import pandas as pd
import numpy as np
import nltk
import string
import re
import gensim
import gensim.downloader as api
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from gensim import corpora, models
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis


nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

# please change the file path accordingly to your needs

df_w=pd.read_csv('/content/western_analysts_tweets.csv', encoding='latin1') #IMPORTANT - THIS FILE HAS LATIN1 ENCODING

df_w.info()

df_r=pd.read_csv('/content/russian_propaganda_tweets.csv', encoding='latin1') #IMPORTANT - THIS FILE HAS LATIN1 ENCODING
#some of the columns were causing issues so they were already removed in the csv file
df_r.info()

"""We can see that for Russian dataset we have a bit more values (22602 compared to 16727 for the Western dataset). So there is a slight imbalance, which might impact the result from the classifier later on."""

#Keeping only the important columns (likes_count and retweets_count might be used only alternatively for a classifier?)
columns_to_keep = ['date', 'tweet', 'likes_count','retweets_count']
df_w = df_w[columns_to_keep]
df_r = df_r[columns_to_keep]

df_w.head()

df_r.head()

#Adding additional column representing whether the post is from Russian perspective or not (0 for western persepective, 1 for Russian)
df_w = df_w.assign(Russian=0)
df_r = df_r.assign(Russian=1)

# From the date column, keep only the month info
df_w['date'] = pd.to_datetime(df_w['date'])
df_w['date'] = df_w['date'].dt.month

print(df_w.head())

df_r['date'] = pd.to_datetime(df_r['date'], format='mixed')
df_r['date'] = df_r['date'].dt.month

df_r.head()

print(df_r.value_counts('date'))
print(df_w.value_counts('date'))

"""With the dates, we can again see there will be some imbalance, as the majority of the tweets are in months 3 and 4, March and April."""

#Checking for missing values
print(df_w.isnull().sum())
print(df_r.isnull().sum())

# No missing values in the dataset

#Preprocess and clean the tweets column

def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    punctuation = set(string.punctuation)
    lemmatizer = WordNetLemmatizer()

    def clean_text(text):
        # Remove URLs
        text = re.sub(r"https?://\S+", "", text)
        # Remove non-alphanumeric characters and extra whitespaces
        text = re.sub(r"[^a-zA-Z\s']", " ", text)
        # Remove extra whitespaces
        text = re.sub(r"\s+", " ", text)
        # Remove hashtags (words starting with #)
        text = re.sub(r"#\w+", "", text)
        # Remove mentions (words starting with @)
        text = re.sub(r"@\w+", "", text)
        # Remove punctuation and special characters (excluding apostrophes)
        text = re.sub(r"[^\w\s']", "", text)
        # Remove newline characters
        text = re.sub(r"[\n\r]", " ", text)
        # Convert to lowercase
        text = text.lower()
        return text

    # Apply the cleaning text function
    cleaned_text = clean_text(text)

    # Tokenize the cleaned text
    tokens = word_tokenize(cleaned_text)

    # Remove stopwords, punctuation, and lemmatize
    new_tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and w not in punctuation and len(w) > 3]

    return (new_tokens)

cleaned_tokens_w = df_w['tweet'].apply(preprocess_text)
cleaned_tokens_r = df_r['tweet'].apply(preprocess_text)

print(cleaned_tokens_w.head(10))

print(cleaned_tokens_r.head(10))

#Appending these cleaned tokens back to the dataframe

df_w['cleaned_tokens'] = df_w['tweet'].apply(preprocess_text)
df_r['cleaned_tokens'] = df_r['tweet'].apply(preprocess_text)

"""# Create BOW representations of words for both datasets"""

# Create a Dictionary object which is mapping unique word tokens to unique IDs
dictionary_w = corpora.Dictionary(cleaned_tokens_w)
print(len(dictionary_w.values()))

dictionary_r = corpora.Dictionary(cleaned_tokens_r)
print(len(dictionary_r.values()))

#Filter out those words tokens that appear in less than 15 documents and more than half of the documents, capping the max at 50000

dictionary_w.filter_extremes(no_below=15, no_above=0.5, keep_n=50000)
print(len(dictionary_w.iteritems()))

dictionary_r.filter_extremes(no_below=15, no_above=0.5, keep_n=50000)
print(len(dictionary_r.iteritems()))

# Convert the list with tokenized tweets to BOW representation
bow_corpus_w = [dictionary_w.doc2bow(tokens) for tokens in cleaned_tokens_w]

bow_corpus_r = [dictionary_r.doc2bow(tokens) for tokens in cleaned_tokens_r]

#Preview some of the BOW representations to get the idea of the format

for i in range(5):
    print("Document {}: {}".format(i+1, bow_corpus_w[i]))

for i in range(5):
    print("Document {}: {}".format(i+1, bow_corpus_r[i]))

#Append the BOW into separate col in our dataframes
bow_scores_w = [dict(doc) for doc in bow_corpus_w]
bow_scores_r = [dict(doc) for doc in bow_corpus_r]

df_w['bow_scores'] = bow_scores_w
df_r['bow_scores'] = bow_scores_r

"""# Create TF IDF representation from the BOW corpus

"""

#TF IDF is a more accurate and precise representation than BOW

tfidf_w = models.TfidfModel(bow_corpus_w)

tfidf_r = models.TfidfModel(bow_corpus_r)

corpus_tfidf_w = tfidf_w[bow_corpus_w]

corpus_tfidf_r = tfidf_r[bow_corpus_r]

# Convert the TF-IDF representations into a list of dictionaries where each dictionary represents TF-IDF scores for a document
tfidf_scores_w = [dict(corpus_tfidf_w[i]) for i in range(len(corpus_tfidf_w))]
tfidf_scores_r = [dict(corpus_tfidf_r[i]) for i in range(len(corpus_tfidf_r))]

# Append the TF-IDF scores as a separate column in the DataFrame
df_w['tfidf_scores'] = tfidf_scores_w
df_r['tfidf_scores'] = tfidf_scores_r

# Print the DataFrame to verify the results
print(df_w.head())
print(df_r.head())

"""# Saving the cleaned datasets, used for other tasks in the project



"""

#Save this dataframe view as new csv for easier reuse
#Note that this saves the df into the session in colab - you need to download the csv for reusal

df_w.to_csv('df_w.csv', index=False)
df_r.to_csv('df_r.csv', index=False)

# Also produce a merged dataframe of the both- which will be mainly used for classification
merged_df = pd.concat([df_w, df_r], ignore_index=True)

print(merged_df.info())

merged_df.to_csv('merged_df.csv', index=False)

"""# Run LDA model with TF IDF (I also tried with BOW, not that much difference)"""

lda_model_w = gensim.models.LdaMulticore(corpus_tfidf_w, num_topics=10,
                                       id2word=dictionary_w, iterations = 5)

lda_model_r = gensim.models.LdaMulticore(corpus_tfidf_r, num_topics=10,
                                       id2word=dictionary_r, iterations = 5)

for idx, topic in lda_model_w.print_topics(-1): # -1 corresponds to all topics
    print('Topic: {} \nWords: {}'.format(idx, topic))

for idx, topic in lda_model_r.print_topics(-1): # -1 corresponds to all topics
    print('Topic: {} \nWords: {}'.format(idx, topic))

!pip install pandas==1.5.3 # THIS VERSION OF PANDAS MUST BE INSTALLED TO BE COMPATIBLE WITH pyLDAvis IN COLAB
!pip install pyLDAvis
!pip install pyLDAvis.gensim

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

vis = gensimvis.prepare(lda_model_w, corpus_tfidf_w, dictionary_w)
pyLDAvis.display(vis)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis


vis = gensimvis.prepare(lda_model_r, corpus_tfidf_r, dictionary_r)
pyLDAvis.display(vis)

"""# Word Clouds (Larissa will adjust this a bit later)"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

from collections import Counter
import itertools


# Calculate document frequency of each word
# Flatten the list of lists into a single list
tokens_flat_w = list(itertools.chain.from_iterable(cleaned_tokens_w))

# Calculate document frequency of each word
word_document_freq = Counter(tokens_flat_w)

# Filter words based on document frequency
filtered_tokens = [token for token in set(tokens_flat_w)
                   if word_document_freq[token] / len(cleaned_tokens_w) <= 0.5]

# Concatenate filtered tokens into a single string
text_w = ' '.join(filtered_tokens)


# Generate word cloud
#all_tokens = [token for tokens in cleaned_tokens_w for token in tokens]
#text_w = ' '.join(all_tokens)

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_w)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Remove axis
plt.show()

# Calculate document frequency of each word
# Flatten the list of lists into a single list
tokens_flat_r = list(itertools.chain.from_iterable(cleaned_tokens_r))

# Calculate document frequency of each word
word_document_freq = Counter(tokens_flat_r)

# Filter words based on document frequency
filtered_tokens = [token for token in set(tokens_flat_r)
                   if word_document_freq[token] / len(cleaned_tokens_r) <= 0.5]

# Concatenate filtered tokens into a single string
text_r = ' '.join(filtered_tokens)


# Generate word cloud
#all_tokens = [token for tokens in cleaned_tokens_w for token in tokens]
#text_w = ' '.join(all_tokens)

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_r)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Remove axis
plt.show()

"""# Some unnecessary code that you dont need to go over
#I kept it in case we might need it for sth later on, but will be probably deleted
"""

# Bag of Words using CountVectorizer
def create_bow(df):
    vectorizer = CountVectorizer()
    X_bow = vectorizer.fit_transform(df['tweet'])
    y_bow = df['encoded_label']
    df['bow_representation'] = list(X_bow.toarray())
    return X_bow, y_bow

# Sentence embeddings by average word vectors
#def average_word_vectors(sentence, model, num_features):
#    average_vector = np.zeros((num_features,), dtype="float64")
#    nwords = 0
#    for word in sentence:
#        if word in model.key_to_index:  #skipping those words which might not be present in the wv model
#            nwords += 1
#            average_vector += model[word]
#    if nwords:
#        average_vector /= nwords
 #   return average_vector
#
#def create_sentence_embeddings(df, model):
 #   df['tokenized_text'] = df['text'].apply(lambda x: x.split())
  #  sentence_vectors = np.array([average_word_vectors(sentence, model, 300)
   #                              for sentence in df['tokenized_text']])
   # df['sentence_vectors'] = sentence_vectors.tolist()
   # return sentence_vectors

# Splitting the dataset into train and test sets, using an 80/20 split
def split_dataset(X, y, test_size=0.2, random_state=42):
    return train_test_split(X, y, test_size=test_size, random_state=random_state)

# Bag of Words
X_bow, y_bow = create_bow(df)
X_train_bow, X_test_bow, y_train_bow, y_test_bow = split_dataset(X_bow, y_bow)

# Sentence embeddings
#sentence_vectors = create_sentence_embeddings(df, wv)
#X_train_vec, X_test_vec, y_train_vec, y_test_vec = split_dataset(sentence_vectors, df['encoded_label'])

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Assuming df_w is your DataFrame with the 'tweet' column

# Define a CountVectorizer for Bag-of-Words representation
count_vectorizer = CountVectorizer(stop_words='english')

# Fit and transform the 'tweet' column to create BOW representation
bow_matrix = count_vectorizer.fit_transform(df_w['tweet'])

# Print the vocabulary (optional)
#print("Vocabulary (Bag-of-Words representation):\n", count_vectorizer.get_feature_names())

# Define a TfidfVectorizer for TF-IDF representation
tfidf_vectorizer = TfidfVectorizer(stop_words='english')

# Fit and transform the 'tweet' column to create TF-IDF representation
tfidf_matrix = tfidf_vectorizer.fit_transform(df_w['tweet'])

# Print the vocabulary (optional)
#print("\nVocabulary (TF-IDF representation):\n", tfidf_vectorizer.get_feature_names())

# Print the shape of the BOW and TF-IDF matrices
print("\nShape of BOW matrix:", bow_matrix.shape)
print("Shape of TF-IDF matrix:", tfidf_matrix.shape)

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Assuming df_w is your DataFrame with the 'tweet' column

# Define a CountVectorizer for Bag-of-Words representation
count_vectorizer = CountVectorizer(stop_words='english')

# Fit and transform the 'tweet' column to create BOW representation
bow_matrix = count_vectorizer.fit_transform(df_w['tweet'])

# Convert BOW matrix to DataFrame and add as new columns
bow_df = pd.DataFrame(bow_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())
df_w = pd.concat([df_w, bow_df], axis=1)

# Define a TfidfVectorizer for TF-IDF representation
tfidf_vectorizer = TfidfVectorizer(stop_words='english')

# Fit and transform the 'tweet' column to create TF-IDF representation
tfidf_matrix = tfidf_vectorizer.fit_transform(df_w['tweet'])

# Convert TF-IDF matrix to DataFrame and add as new columns
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
df_w = pd.concat([df_w, tfidf_df], axis=1)

# Print the updated DataFrame
print(df_w.head())

# Using Multinomial NB from scikit for the binary classification, and printing the result

bayes_classifier = MultinomialNB()
bayes_classifier.fit(X_train_bow, y_train_bow)
y_pred_bayes = bayes_classifier.predict(X_test_bow)

report = classification_report(y_test_bow, y_pred_bayes)
print(report)

# Creating the Logistic Regression model, setting high maximum iteration number
# to avoid the following: 'ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.'
logistic_regr_model = LogisticRegression(max_iter=500)

# We will use the same split into training and test data as in 2.1.
# Training the model (using training data)
logistic_regr_model.fit(X_train_bow, y_train_bow)

# Making predictions (using test data)
y_pred_logistic = logistic_regr_model.predict(X_test_bow)

report = classification_report(y_test_bow, y_pred_bayes)
print(report)