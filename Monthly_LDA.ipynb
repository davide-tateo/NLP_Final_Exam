{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Month-on-Month Topic Modelling using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilizador\\AppData\\Local\\Programs\\Microsoft VS Code\n"
     ]
    }
   ],
   "source": [
    "# Checking the current working directory to download the files\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique months and respective counts in Russian tweets dataset:\n",
      "2    1612\n",
      "3    8960\n",
      "4    9722\n",
      "5    2308\n",
      "Name: month, dtype: int64\n",
      "\n",
      "Unique months and respective counts in Western tweets dataset:\n",
      "2    1780\n",
      "3    7501\n",
      "4    6179\n",
      "5    1267\n",
      "Name: month, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking date columns of the preprocessed .csv\n",
    "\n",
    "  # 1) Reading both .csv files\n",
    "df_r = pd.read_csv('df_r.csv')\n",
    "df_w = pd.read_csv('df_w.csv')\n",
    "\n",
    "  # 2) Getting the unique date values from each dataset\n",
    "months_df_r = df_r['month'].value_counts().sort_index()\n",
    "months_df_w = df_w['month'].value_counts().sort_index()\n",
    "\n",
    "  # 3) Printing results\n",
    "print(\"Unique months and respective counts in Russian tweets dataset:\")\n",
    "print(months_df_r)\n",
    "\n",
    "print(\"\\nUnique months and respective counts in Western tweets dataset:\")\n",
    "print(months_df_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four common months in both datasets: 2 (February), 3 (March), 4 (April), and 5 (May). We will consider these four months for further monthly LDA comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique months in the Feb Russian tweets dataset: [2]\n",
      "Unique months in the March Russian tweets dataset: [3]\n",
      "Unique months in the April Russian tweets dataset: [4]\n",
      "Unique months in the May Russian tweets dataset: [5]\n",
      "\n",
      "Unique months in the Feb Western tweets dataset: [2]\n",
      "Unique months in the March Western tweets dataset: [3]\n",
      "Unique months in the April Western tweets dataset: [4]\n",
      "Unique months in the May Western tweets dataset: [5]\n"
     ]
    }
   ],
   "source": [
    "# Splitting each of the datasets into three new files, based on the month of the tweet\n",
    "\n",
    "df_r_2 = df_r[df_r['month'] == 2]\n",
    "df_r_3 = df_r[df_r['month'] == 3]\n",
    "df_r_4 = df_r[df_r['month'] == 4]\n",
    "df_r_5 = df_r[df_r['month'] == 5]\n",
    "\n",
    "df_w_2 = df_w[df_w['month'] == 2]\n",
    "df_w_3 = df_w[df_w['month'] == 3]\n",
    "df_w_4 = df_w[df_w['month'] == 4]\n",
    "df_w_5 = df_w[df_w['month'] == 5]\n",
    "\n",
    "# Checking the month values of the new dataframes to confirm if the split was effective\n",
    "print(\"Unique months in the Feb Russian tweets dataset:\", df_r_2['month'].unique())\n",
    "print(\"Unique months in the March Russian tweets dataset:\", df_r_3['month'].unique())\n",
    "print(\"Unique months in the April Russian tweets dataset:\", df_r_4['month'].unique())\n",
    "print(\"Unique months in the May Russian tweets dataset:\", df_r_5['month'].unique())\n",
    "print(\"\")\n",
    "print(\"Unique months in the Feb Western tweets dataset:\", df_w_2['month'].unique())\n",
    "print(\"Unique months in the March Western tweets dataset:\", df_w_3['month'].unique())\n",
    "print(\"Unique months in the April Western tweets dataset:\", df_w_4['month'].unique())\n",
    "print(\"Unique months in the May Western tweets dataset:\", df_w_5['month'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed that splitting the datasets based on date was done successfully, we can save them as separate .csv files so they can be further used for new LDA monthly comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the split datasets as new .csv files to be easily accessible for further LDA analysis\n",
    "\n",
    "df_r_2.to_csv('df_r_2.csv', index = False)\n",
    "df_r_3.to_csv('df_r_3.csv', index = False)\n",
    "df_r_4.to_csv('df_r_4.csv', index = False)\n",
    "df_r_5.to_csv('df_r_5.csv', index = False)\n",
    "\n",
    "df_w_2.to_csv('df_w_2.csv', index = False)\n",
    "df_w_3.to_csv('df_w_3.csv', index = False)\n",
    "df_w_4.to_csv('df_w_4.csv', index = False)\n",
    "df_w_5.to_csv('df_w_5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the Feb Russian tweets dataset: 1612\n",
      "Number of rows in the March Russian tweets dataset: 8960\n",
      "Number of rows in the April Russian tweets dataset: 9722\n",
      "Number of rows in the May Russian tweets dataset: 2308\n",
      "\n",
      "Number of rows in the Feb Western tweets dataset: 1780\n",
      "Number of rows in the March Western tweets dataset: 7501\n",
      "Number of rows in the April Western tweets dataset: 6179\n",
      "Number of rows in the May Western tweets dataset: 1267\n"
     ]
    }
   ],
   "source": [
    "# Checking if the new .csv files were successfuly saved\n",
    "\n",
    "df_r_2 = pd.read_csv('df_r_2.csv')\n",
    "df_r_3 = pd.read_csv('df_r_3.csv')\n",
    "df_r_4 = pd.read_csv('df_r_4.csv')\n",
    "df_r_5 = pd.read_csv('df_r_5.csv')\n",
    "\n",
    "df_w_2 = pd.read_csv('df_w_2.csv')\n",
    "df_w_3 = pd.read_csv('df_w_3.csv')\n",
    "df_w_4 = pd.read_csv('df_w_4.csv')\n",
    "df_w_5 = pd.read_csv('df_w_5.csv')\n",
    "\n",
    "# Checking if the number of rows in the new dataframes match with the value counts of each month\n",
    "print(\"Number of rows in the Feb Russian tweets dataset:\", df_r_2.shape[0])\n",
    "print(\"Number of rows in the March Russian tweets dataset:\", df_r_3.shape[0])\n",
    "print(\"Number of rows in the April Russian tweets dataset:\", df_r_4.shape[0])\n",
    "print(\"Number of rows in the May Russian tweets dataset:\", df_r_5.shape[0])\n",
    "print(\"\")\n",
    "print(\"Number of rows in the Feb Western tweets dataset:\", df_w_2.shape[0])\n",
    "print(\"Number of rows in the March Western tweets dataset:\", df_w_3.shape[0])\n",
    "print(\"Number of rows in the April Western tweets dataset:\", df_w_4.shape[0])\n",
    "print(\"Number of rows in the May Western tweets dataset:\", df_w_5.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows in each of the new .csv files matches the value counts of each month in the preprocessed dataframes, indicating that the new files are now ready to be reused in the LDA analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand if there was a MoM evolution of the topics discussed, we will perform topic modelling using LDA on each of the newly created dataframes (representing the tweets posted in each month). To do so, we need to create separate TF-IDF corpus for each month's data (we opted for TF-IDF because it holds more information on the more/less important words, and thus should ensure a higher accuracy of the derived insights).\n",
    "Since the new .csv files were created from the already preprocessed dataframes, we will use the cleaned_tokens column to create the separate TF-IDF corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new monthly TF-IDF corpus to pass onto the LDA model\n",
    "\n",
    "  ## 1) Adding the cleaned_tokens entries for each month into a list\n",
    "cleaned_tokens_r_2 = df_r_2['cleaned_tokens'].tolist()\n",
    "cleaned_tokens_r_3 = df_r_3['cleaned_tokens'].tolist()\n",
    "cleaned_tokens_r_4 = df_r_4['cleaned_tokens'].tolist()\n",
    "cleaned_tokens_r_5 = df_r_5['cleaned_tokens'].tolist()\n",
    "\n",
    "cleaned_tokens_w_2 = df_w_2['cleaned_tokens'].tolist()\n",
    "cleaned_tokens_w_3 = df_w_3['cleaned_tokens'].tolist()\n",
    "cleaned_tokens_w_4 = df_w_4['cleaned_tokens'].tolist()\n",
    "cleaned_tokens_w_5 = df_w_5['cleaned_tokens'].tolist()\n",
    "\n",
    "# The result is a list of lists (the cleaned_tokens of each tweet [row] are added to the cleaned_tokens_x_y list)\n",
    "\n",
    "  ## 2) Initializing a separate TF-IDF Vectorizer for each side (Russian vs. Western)\n",
    "tfidf_vectorizer_r = TfidfVectorizer()\n",
    "tfidf_vectorizer_w = TfidfVectorizer()\n",
    "\n",
    "# Aggregating tokens by side, so that we can fit the vectorizers on the aggregated data\n",
    "cleaned_tokens_r = (cleaned_tokens_r_2 + cleaned_tokens_r_3 + cleaned_tokens_r_4 + cleaned_tokens_r_5)\n",
    "cleaned_tokens_w = (cleaned_tokens_w_2 + cleaned_tokens_w_3 + cleaned_tokens_w_4 + cleaned_tokens_w_5)\n",
    "\n",
    "\n",
    "  ## 3) Fitting the TF-IDF Vectorizers on side's data\n",
    "tfidf_corpus_r = tfidf_vectorizer_r.fit_transform(cleaned_tokens_r)\n",
    "tfidf_corpus_w = tfidf_vectorizer_w.fit_transform(cleaned_tokens_w)\n",
    "  \n",
    "  ## 4) Building the corpora with each month's data to be used in LDA\n",
    "tfidf_corpus_r_2 = tfidf_vectorizer_r.transform(cleaned_tokens_r_2)\n",
    "tfidf_corpus_r_3 = tfidf_vectorizer_r.transform(cleaned_tokens_r_3)\n",
    "tfidf_corpus_r_4 = tfidf_vectorizer_r.transform(cleaned_tokens_r_4)\n",
    "tfidf_corpus_r_5 = tfidf_vectorizer_r.transform(cleaned_tokens_r_5)\n",
    "\n",
    "tfidf_corpus_w_2 = tfidf_vectorizer_w.transform(cleaned_tokens_w_2)\n",
    "tfidf_corpus_w_3 = tfidf_vectorizer_w.transform(cleaned_tokens_w_3)\n",
    "tfidf_corpus_w_4 = tfidf_vectorizer_w.transform(cleaned_tokens_w_4)\n",
    "tfidf_corpus_w_5 = tfidf_vectorizer_w.transform(cleaned_tokens_w_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Obtaining dependency information for pyLDAvis from https://files.pythonhosted.org/packages/6b/5a/66364c6799f2362bfb9b7100bc1ce6ffcdfe7f17e8d2e85a591bfe427643/pyLDAvis-3.4.1-py3-none-any.whl.metadata\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.10.1)\n",
      "Collecting pandas>=2.0.0 (from pyLDAvis)\n",
      "  Obtaining dependency information for pandas>=2.0.0 from https://files.pythonhosted.org/packages/ab/63/966db1321a0ad55df1d1fe51505d2cdae191b84c907974873817b0a6e849/pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: numexpr in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.4)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Obtaining dependency information for funcy from https://files.pythonhosted.org/packages/d5/08/c2409cb01d5368dcfedcbaffa7d044cc8957d57a9d0855244a5eb4709d30/funcy-2.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pyLDAvis) (68.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2022.7)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.0.0->pyLDAvis)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim->pyLDAvis)\n",
      "  Obtaining dependency information for FuzzyTM>=0.4.0 from https://files.pythonhosted.org/packages/2d/30/074bac7a25866a2807c1005c7852c0139ac22ba837871fc01f16df29b9dc/FuzzyTM-2.0.9-py3-none-any.whl.metadata\n",
      "  Downloading FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Obtaining dependency information for pyfume from https://files.pythonhosted.org/packages/ed/ea/a3b120e251145dcdb10777f2bc5f18b1496fd999d705a178c1b0ad947ce1/pyFUME-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Collecting numpy>=1.24.2 (from pyLDAvis)\n",
      "  Obtaining dependency information for numpy>=1.24.2 from https://files.pythonhosted.org/packages/d8/ec/ebef2f7d7c28503f958f0f8b992e7ce606fb74f9e891199329d5f5f87404/numpy-1.24.4-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.24.4-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Obtaining dependency information for simpful==2.12.0 from https://files.pythonhosted.org/packages/9d/0e/aebc2fb0b0f481994179b2ee2b8e6bbf0894d971594688c018375e7076ea/simpful-2.12.0-py3-none-any.whl.metadata\n",
      "  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of pyfume to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Obtaining dependency information for pyfume from https://files.pythonhosted.org/packages/f0/fe/b899a3d9a18c9a44a35155c79a4c152cb85990ea38ce6ab7ed73e5caa1b9/pyFUME-0.3.1-py3-none-any.whl.metadata\n",
      "  Downloading pyFUME-0.3.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (4.7.1)\n",
      "Collecting miniful (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/2.6 MB 5.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.4/2.6 MB 5.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.8/2.6 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.6 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.9/2.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.2/2.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.5/2.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/11.6 MB 6.5 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.5/11.6 MB 6.7 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.8/11.6 MB 6.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.1/11.6 MB 6.3 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.4/11.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.7/11.6 MB 6.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.0/11.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.3/11.6 MB 6.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.6/11.6 MB 6.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/11.6 MB 6.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.6/11.6 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/11.6 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.2/11.6 MB 6.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.4/11.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.7/11.6 MB 6.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.9/11.6 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.3/11.6 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.6/11.6 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.9/11.6 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.2/11.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.6/11.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.9/11.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.3/11.6 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.6/11.6 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.8/11.6 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.1/11.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.4/11.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.0/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.3/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.6/11.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.9/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.8/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Downloading FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   ---------------------------------------- 0.0/345.4 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 317.4/345.4 kB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 345.4/345.4 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading pyFUME-0.3.1-py3-none-any.whl (59 kB)\n",
      "   ---------------------------------------- 0.0/59.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 59.6/59.6 kB ? eta 0:00:00\n",
      "Downloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20448 sha256=baf1844d94dbdfada415d10ce75fc6de8fcf6017c50ec1763c1a90da6e4ed29d\n",
      "  Stored in directory: c:\\users\\utilizador\\appdata\\local\\pip\\cache\\wheels\\69\\f5\\e5\\18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3522 sha256=3e2c190d1194ca8d47b4b0e88857276b4601fb617da7b28468d68067d233d4d1\n",
      "  Stored in directory: c:\\users\\utilizador\\appdata\\local\\pip\\cache\\wheels\\9d\\ff\\2f\\afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: funcy, tzdata, simpful, pandas, miniful, fst-pso, pyfume, FuzzyTM, pyLDAvis\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acesso negado: 'C:\\\\Users\\\\Utilizador\\\\anaconda3\\\\Lib\\\\site-packages\\\\~andas\\\\_libs\\\\algos.cp311-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyLDAvis.gensim (from versions: none)\n",
      "ERROR: No matching distribution found for pyLDAvis.gensim\n"
     ]
    }
   ],
   "source": [
    "# Instaling the necessary packages and libraries\n",
    "%pip install pandas==1.5.3 # THIS VERSION OF PANDAS MUST BE INSTALLED TO BE COMPATIBLE WITH pyLDAvis\n",
    "%pip install pyLDAvis\n",
    "%pip install pyLDAvis.gensim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
